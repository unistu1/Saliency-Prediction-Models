# Saliency-Prediction-Models
Comprehensive List of Visual Saliency Prediction Models and GitHub Repositories
# Comprehensive List of Visual Saliency Prediction Models and GitHub Repositories

Below is a curated list of leading papers in visual saliency prediction along with their corresponding GitHub repositories and descriptions:

## 1. Contextual Encoder-Decoder Network for Visual Saliency Prediction
- **Paper**: Contextual Encoder-Decoder Network for Visual Saliency Prediction
- **Code Repository**: [GitHub - saliency](https://github.com/alexanderkroner/saliency)
- **Description**: This repository contains the implementation of the Contextual Encoder-Decoder Network designed to predict visual saliency in images. The model leverages contextual information to enhance the accuracy of saliency maps, providing a robust framework for understanding human attention in visual scenes.

## 2. Predicting Human Eye Fixations via an LSTM-based Saliency Attentive Model
- **Paper**: Predicting Human Eye Fixations via an LSTM-based Saliency Attentive Model
- **Code Repository**: [GitHub - sam](https://github.com/marcellacornia/sam)
- **Description**: This repository implements an LSTM-based Saliency Attentive Model (SAM) aimed at predicting human eye fixations. By integrating Long Short-Term Memory networks, the model captures temporal dependencies in visual data, improving the prediction of where humans are likely to focus their attention.

## 3. TranSalNet: Towards Perceptually Relevant Visual Saliency Prediction
- **Paper**: TranSalNet: Towards Perceptually Relevant Visual Saliency Prediction
- **Code Repository**: [GitHub - TranSalNet](https://github.com/LJOVO/TranSalNet)
- **Description**: TranSalNet introduces a Transformer-based architecture for visual saliency prediction. This repository provides the implementation of TranSalNet, which leverages the attention mechanisms of Transformers to achieve perceptually relevant saliency maps, enhancing the understanding of human visual attention.

## 4. GazeGAN: A New Saliency Prediction Model
- **Paper**: GazeGAN: A New Saliency Prediction Model
- **Code Repository**: [GitHub - Sal-CFS-GAN](https://github.com/CZHQuality/Sal-CFS-GAN)
- **Description**: GazeGAN employs Generative Adversarial Networks (GANs) for saliency prediction. The repository includes the implementation of the Sal-CFS-GAN model, which focuses on generating high-quality saliency maps by exploiting the adversarial training framework to mimic human gaze patterns accurately.

## 5. A Dilated Inception Network for Visual Saliency Prediction
- **Paper**: A Dilated Inception Network for Visual Saliency Prediction
- **Code Repository**: [GitHub - DINet](https://github.com/ysyscool/DINet)
- **Description**: DINet utilizes a Dilated Inception architecture to enhance visual saliency prediction. This repository provides the code for DINet, which integrates dilated convolutions within the Inception modules to capture multi-scale contextual information, thereby improving the precision of saliency maps.

## 6. A Model of Saliency-Based Visual Attention for Rapid Scene Analysis
- **Paper**: A Model of Saliency-Based Visual Attention for Rapid Scene Analysis
- **Code Repository**: [GitHub - gbvs](https://github.com/shreelock/gbvs)
- **Description**: This repository hosts the implementation of the Graph-Based Visual Saliency (GBVS) model. GBVS is designed for rapid scene analysis by modeling visual attention through graph-based representations, allowing for efficient and effective saliency map generation.

## 7. RINet: Relative Importance-Aware Network for Fixation Prediction
- **Paper**: RINet: Relative Importance-Aware Network for Fixation Prediction
- **Code Repository**: [GitHub - TranSalNet](https://github.com/LJOVO/TranSalNet)
- **Description**: RINet focuses on predicting fixation points by assessing the relative importance of different regions within an image. Although hosted under the TranSalNet repository, RINet extends the capabilities of TranSalNet by incorporating mechanisms that prioritize salient regions based on their relative importance.

## 8. A Model of Saliency-Based Visual Attention for Rapid Scene Analysis
- **Paper**: A Model of Saliency-Based Visual Attention for Rapid Scene Analysis
- **Code Repository**: [GitHub - Itti](https://github.com/FenHua/Itti)
- **Description**: This repository contains the implementation of Itti's model of saliency-based visual attention. The model simulates human visual attention by detecting salient features in scenes, enabling rapid and accurate scene analysis through computational methods.

## 9. A Deep Multi-Level Network for Saliency Prediction
- **Paper**: A Deep Multi-Level Network for Saliency Prediction
- **Code Repository**: [GitHub - mlnet](https://github.com/marcellacornia/mlnet)
- **Description**: MLNet presents a deep multi-level architecture for saliency prediction, integrating multiple layers of feature extraction to enhance saliency map accuracy. The repository provides the necessary code to train and evaluate the MLNet model, which excels in capturing complex visual patterns.

## 10. A Gated Fusion Network for Dynamic Saliency Prediction
- **Paper**: A Gated Fusion Network for Dynamic Saliency Prediction
- **Code Repository**: [GitHub - GFSalNet](https://hucvl.github.io/GFSalNet/)
- **Description**: GFSalNet introduces a Gated Fusion Network designed for dynamic saliency prediction. This repository includes the implementation of the model, which effectively fuses information from multiple sources using gating mechanisms to adaptively predict saliency in changing visual environments.

## 11. Exploiting Inter-image Similarity and Ensemble of Extreme Learners for Fixation Prediction Using Deep Features
- **Paper**: Exploiting Inter-image Similarity and Ensemble of Extreme Learners for Fixation Prediction Using Deep Features
- **Code Repository**: [GitHub - iseel](https://github.com/hrtavakoli/iseel)
- **Description**: The ISEEL repository implements a model that leverages inter-image similarity and an ensemble of extreme learners to predict fixation points using deep features. This approach enhances fixation prediction by combining multiple learning strategies and exploiting similarities across different images.

## 12. Vision Transformers for Dense Prediction
- **Paper**: Vision Transformers for Dense Prediction
- **Code Repository**: [GitHub - DPT](https://github.com/isl-org/DPT)
- **Description**: DPT (Dense Prediction Transformers) applies Vision Transformers to dense prediction tasks, including saliency prediction. This repository provides the implementation of DPT, which utilizes transformer architectures to achieve high-performance results in dense visual tasks by capturing long-range dependencies in images.

## 13. Weakly Supervised Visual Saliency Prediction
- **Paper**: Weakly Supervised Visual Saliency Prediction
- **Code Repository**: [GitHub - WeakFixation](https://github.com/ashleylqx/WeakFixation)
- **Description**: WeakFixation focuses on saliency prediction using weak supervision techniques. The repository includes the implementation of models that learn to predict visual saliency without requiring extensive labeled data, making the approach more scalable and applicable to diverse datasets.
- 
## 14. Deep Visual Attention Prediction
- **Paper**: Deep Visual Attention Prediction
- **Code Repository**:  GitHub - Deep-Visual-Attention-Prediction
- **Description**:This repository implements a model focused on deep visual attention prediction. It leverages advanced deep learning techniques to predict attention maps that closely align with human fixation patterns in visual tasks, providing insights into attention mechanisms in computer vision.
---

## Notes

- **Paper Links**: The provided paper titles are listed without direct links. For access to the full papers, it is recommended to search for the titles on platforms like [Google Scholar](https://scholar.google.com/), [arXiv](https://arxiv.org/), or relevant academic journals.
  
- **Code Repositories**: Each GitHub link contains the implementation of the respective models. These repositories often include additional resources such as pre-trained models, datasets, and usage instructions.

## Usage Recommendations

- **Exploring Implementations**: Clone the GitHub repositories to experiment with the models. This hands-on approach can deepen your understanding of visual saliency prediction techniques.
  
  ```bash
  git clone https://github.com/alexanderkroner/saliency.git
  git clone https://github.com/marcellacornia/sam.git
  git clone https://github.com/LJOVO/TranSalNet.git
  git clone https://github.com/CZHQuality/Sal-CFS-GAN.git
  git clone https://github.com/ysyscool/DINet.git
  git clone https://github.com/shreelock/gbvs.git
  git clone https://github.com/FenHua/Itti.git
  git clone https://github.com/marcellacornia/mlnet.git
  git clone https://hucvl.github.io/GFSalNet/
  git clone https://github.com/hrtavakoli/iseel.git
  git clone https://github.com/isl-org/DPT.git
  git clone https://github.com/ashleylqx/WeakFixation.git
  https://github.com/suraj-maniyar/Deep-Visual-Attention-Prediction
